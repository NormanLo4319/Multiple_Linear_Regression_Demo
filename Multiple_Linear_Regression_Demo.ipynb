{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "handed-wisdom",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "In this demo, we are going to review and practice using the multiple linear regression to extract some meaningful insight from various data sets.\n",
    "\n",
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Multiple-Linear-Regression\" data-toc-modified-id=\"Multiple-Linear-Regression-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Multiple Linear Regression</a></span></li><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#Regression-with-Multiple-Predictors\" data-toc-modified-id=\"Regression-with-Multiple-Predictors-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Regression with Multiple Predictors</a></span><ul class=\"toc-item\"><li><span><a href=\"#Expanding-Simple-Linear-Regression\" data-toc-modified-id=\"Expanding-Simple-Linear-Regression-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Expanding Simple Linear Regression</a></span></li><li><span><a href=\"#Closed-form-Solution\" data-toc-modified-id=\"Closed-form-Solution-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Closed-form Solution</a></span></li></ul></li><li><span><a href=\"#Confounding-Variables\" data-toc-modified-id=\"Confounding-Variables-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Confounding Variables</a></span></li><li><span><a href=\"#Dealing-with-Categorical-Variables\" data-toc-modified-id=\"Dealing-with-Categorical-Variables-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Dealing with Categorical Variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dummying\" data-toc-modified-id=\"Dummying-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Dummying</a></span></li></ul></li><li><span><a href=\"#Multiple-Regression-in-statsmodels\" data-toc-modified-id=\"Multiple-Regression-in-statsmodels-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Multiple Regression in <code>statsmodels</code></a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Diamonds-Dataset\" data-toc-modified-id=\"Diamonds-Dataset-6.0.1\"><span class=\"toc-item-num\">6.0.1&nbsp;&nbsp;</span>Diamonds Dataset</a></span></li><li><span><a href=\"#Check-distribution-of-target\" data-toc-modified-id=\"Check-distribution-of-target-6.0.2\"><span class=\"toc-item-num\">6.0.2&nbsp;&nbsp;</span>Check distribution of target</a></span></li><li><span><a href=\"#Build-model-with-log-scaled-target\" data-toc-modified-id=\"Build-model-with-log-scaled-target-6.0.3\"><span class=\"toc-item-num\">6.0.3&nbsp;&nbsp;</span>Build model with log-scaled target</a></span></li></ul></li></ul></li><li><span><a href=\"#Putting-it-in-Practice:-Wine-Dataset-üç∑\" data-toc-modified-id=\"Putting-it-in-Practice:-Wine-Dataset-üç∑-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Putting it in Practice: Wine Dataset üç∑</a></span><ul class=\"toc-item\"><li><span><a href=\"#üß†-Knowledge-Check\" data-toc-modified-id=\"üß†-Knowledge-Check-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>üß† <strong>Knowledge Check</strong></a></span></li><li><span><a href=\"#Running-the-Regression\" data-toc-modified-id=\"Running-the-Regression-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Running the Regression</a></span></li></ul></li><li><span><a href=\"#Scaling---The-Missing-&amp;-Helpful-Step\" data-toc-modified-id=\"Scaling---The-Missing-&amp;-Helpful-Step-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Scaling - The Missing &amp; Helpful Step</a></span><ul class=\"toc-item\"><li><span><a href=\"#What's-Going-on-Here?\" data-toc-modified-id=\"What's-Going-on-Here?-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>What's Going on Here?</a></span></li><li><span><a href=\"#A-Solution:-Standard-Scaling\" data-toc-modified-id=\"A-Solution:-Standard-Scaling-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>A Solution: Standard Scaling</a></span></li><li><span><a href=\"#Redoing-with-Standard-Scaling\" data-toc-modified-id=\"Redoing-with-Standard-Scaling-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Redoing with Standard Scaling</a></span></li><li><span><a href=\"#üß†-Knowledge-Check\" data-toc-modified-id=\"üß†-Knowledge-Check-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>üß† <strong>Knowledge Check</strong></a></span></li><li><span><a href=\"#üß†-Knowledge-Check\" data-toc-modified-id=\"üß†-Knowledge-Check-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>üß† <strong>Knowledge Check</strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#Follow-Up\" data-toc-modified-id=\"Follow-Up-8.5.1\"><span class=\"toc-item-num\">8.5.1&nbsp;&nbsp;</span>Follow-Up</a></span></li></ul></li></ul></li><li><span><a href=\"#Multiple-Regression-in-Scikit-Learn\" data-toc-modified-id=\"Multiple-Regression-in-Scikit-Learn-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Multiple Regression in Scikit-Learn</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scale-the-Data\" data-toc-modified-id=\"Scale-the-Data-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Scale the Data</a></span></li><li><span><a href=\"#Fit-the-Model\" data-toc-modified-id=\"Fit-the-Model-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Fit the Model</a></span></li><li><span><a href=\"#Evaluate-Performance\" data-toc-modified-id=\"Evaluate-Performance-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Evaluate Performance</a></span><ul class=\"toc-item\"><li><span><a href=\"#Observing-Residuals\" data-toc-modified-id=\"Observing-Residuals-9.3.1\"><span class=\"toc-item-num\">9.3.1&nbsp;&nbsp;</span>Observing Residuals</a></span></li><li><span><a href=\"#Sklearn-Metrics\" data-toc-modified-id=\"Sklearn-Metrics-9.3.2\"><span class=\"toc-item-num\">9.3.2&nbsp;&nbsp;</span>Sklearn Metrics</a></span></li><li><span><a href=\"#More-in-Exploring-of-the-Predictions\" data-toc-modified-id=\"More-in-Exploring-of-the-Predictions-9.3.3\"><span class=\"toc-item-num\">9.3.3&nbsp;&nbsp;</span>More in Exploring of the Predictions</a></span></li></ul></li></ul></li><li><span><a href=\"#Level-Up:-Deeper-Evaluation-of-Wine-Data-Predictions\" data-toc-modified-id=\"Level-Up:-Deeper-Evaluation-of-Wine-Data-Predictions-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Level Up: Deeper Evaluation of Wine Data Predictions</a></span></li><li><span><a href=\"#Level-Up:-Regression-with-Categorical-Features-with-the-Comma-Dataset\" data-toc-modified-id=\"Level-Up:-Regression-with-Categorical-Features-with-the-Comma-Dataset-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Level Up: Regression with Categorical Features with the Comma Dataset</a></span></li></ul></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-basement",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-window",
   "metadata": {},
   "source": [
    "![mlr](https://miro.medium.com/max/1280/1*lJKFo3yyZaFIx4ET1dLmlg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-defensive",
   "metadata": {},
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-relations",
   "metadata": {},
   "source": [
    "- Use the one-hot strategy to encode categorical variables\n",
    "- Conduct linear regressions in `statsmodels`\n",
    "- Use standard scaling for linear regression for better  interpretation\n",
    "- Conduct linear regressions in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-knitting",
   "metadata": {},
   "source": [
    "# Regression with Multiple Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-purchase",
   "metadata": {},
   "source": [
    "> It's all a bunch of dials\n",
    "\n",
    "<img width='450px' src='img/dials.png'/>\n",
    "\n",
    "The main idea here is pretty simple. Whereas, in simple linear regression we took our dependent variable to be a function only of a single independent variable, here we'll be taking the dependent variable to be a function of multiple independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-temple",
   "metadata": {},
   "source": [
    "## Expanding Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-coordination",
   "metadata": {},
   "source": [
    "Our regression equation, then, instead of looking like $\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$, will now look like:\n",
    "\n",
    "$$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + ... + \\hat{\\beta}_nx_n$$\n",
    "\n",
    "Remember that the hats ( $\\hat{}$ ) indicate parameters that are estimated.\n",
    "\n",
    "Is this still a best-fit *line*? Well, no. What does the graph of, say, z = x + y look like? [Here's](https://academo.org/demos/3d-surface-plotter/) a 3d-plotter. (Of course, once we get x's with subscripts beyond 2 it's going to be very hard to visualize. But in practice linear regressions can make use of dozens or even of hundreds of independent variables!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-device",
   "metadata": {},
   "source": [
    "## Closed-form Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-color",
   "metadata": {},
   "source": [
    "Is it possible to calculate the $\\beta$s by hand? Yes, a multiple regression problem still has a closed-form solution.\n",
    "\n",
    "In a word, for a multiple linear regression problem where $X$ is the matrix of independent variable values and $y$ is the vector of dependent variable values, the vector of optimizing regression coefficients $\\vec{b}$ is given by:\n",
    "\n",
    "$$\\vec{b} = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "We'll focus more directly on matrix mathematics later in the course, so don't worry if this equation is opaque to you. See [here](https://stattrek.com/multiple-regression/regression-coefficients.aspx) for a nice explanation and example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-python",
   "metadata": {},
   "source": [
    "# Confounding Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-stadium",
   "metadata": {},
   "source": [
    "Suppose I have a simple linear regression that models the growth of corn plants as a function of the temperature of the ambient air. And suppose there is a noticeable positive correlation between temperature and plant height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'corn' data set\n",
    "corn = pd.read_csv('data/corn.csv', \n",
    "                   usecols=['temp', 'humid', 'height'])\n",
    "\n",
    "# Extract the first 5 rows of the data\n",
    "corn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot corn height against temperature with regression line\n",
    "sns.lmplot(data=corn, x='temp', y='height')\n",
    "plt.xlabel('Temperature ($\\degree$ F)')\n",
    "plt.ylabel('Height (cm)')\n",
    "plt.title('Corn plant height as a function of temperature');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-replication",
   "metadata": {},
   "source": [
    "It seems that higher temperatures lead to taller corn plants. But it's hard to know for sure. One **confounding variable** might be *humidity*. If we haven't controlled for humidity, then it's difficult to draw conclusions.\n",
    "\n",
    "One solution is to use **both features** in a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot corn height against humidity with regression line\n",
    "sns.lmplot(data=corn, x='humid', y='height')\n",
    "plt.xlabel('Humidity (%)')\n",
    "plt.ylabel('Height (cm)')\n",
    "plt.title('Corn plant height as a function of humidity');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-chocolate",
   "metadata": {},
   "source": [
    "When we are using two independent variables in multiple linear regression model, we are fitting a hyperplane / surface to the data in a 3 dimensional space. Here is the visualization of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 3D figure for plotting the data and surface\n",
    "ax = plt.figure(figsize=(8, 6)).add_subplot(111, projection='3d')\n",
    "ax.scatter(corn['temp'], corn['humid'], corn['height'],\n",
    "           depthshade=True, s=40, color='#ff0000')\n",
    "# create x,y\n",
    "xx, yy = np.meshgrid(corn['temp'], corn['humid'])\n",
    "\n",
    "# calculate corresponding z\n",
    "z = 4.3825 * xx + 2.4693 * yy - 255.5434\n",
    "\n",
    "# plot the surface\n",
    "ax.plot_surface(xx, yy, z, alpha=0.01, color='#00ff00')\n",
    "\n",
    "ax.view_init(30, azim=240)\n",
    "ax.set_xlabel('Temperature ($\\degree$ F)')\n",
    "ax.set_ylabel('Humidity (%)')\n",
    "ax.set_zlabel('Height (cm)')\n",
    "plt.title('Corn plant height as a function of temperature and humidity');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-strain",
   "metadata": {},
   "source": [
    "One risk we run when adding more predictors to a model is that their correlations with the target may be nearly *collinear* with each other. This can make it difficult to determine which predictor is doing the heavy lifting. We shall explore this theme of **multicollinearity** in more depth in due course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-clone",
   "metadata": {},
   "source": [
    "# Dealing with Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-engagement",
   "metadata": {},
   "source": [
    "One issue we'd like to resolve is what to do with categorical variables, i.e. variables that represent categories rather than continua. In a Pandas DataFrame, these columns may well have strings or objects for values, but they need not. A certain heart-disease dataset from Kaggle, for example, has a target variable that takes values 0-4, each representing a different stage of heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-alfred",
   "metadata": {},
   "source": [
    "## Dummying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-cutting",
   "metadata": {},
   "source": [
    "One effective way of dealing with categorical variables is to dummy them out. What this involves is making a new column for *each categorical value (level) in the column we're dummying out*.\n",
    "\n",
    "These new columns will be filled only with 0's and 1's, a 1 representing the presence of the relevant categorical value (level).\n",
    "\n",
    "Example:\n",
    "\n",
    "| User_ID | Color |\n",
    "| :---: | :---: |\n",
    "| AD154 | Blue |\n",
    "| CE439 | Green |\n",
    "| TS990 | Blue |\n",
    "| DE468 | Red |\n",
    "\n",
    "Transform into dummy columns:\n",
    "\n",
    "| User_ID | Blue | Green | Red |\n",
    "| :---: | :---: | :---: | :---: |\n",
    "| AD154 |  1 | 0 | 0 |\n",
    "| CE439 |  0 | 1 | 0 |\n",
    "| TS990 |  1 | 0 | 0 |\n",
    "| DE468 |  0 | 0 | 1 |\n",
    "\n",
    "**Note**: When using categorical variable in a regression model, we include k - 1 level(s) and leave one out as the **reference level**. In the previous example, we can include the columns 'Blue' and 'Green' in the model and leave the 'Red' column out. The reference level can be any column between the three levels.\n",
    "\n",
    "Let's look at a simple example with the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'comma-survey' data set\n",
    "comma_use = pd.read_csv('data/comma-survey.csv')\n",
    "\n",
    "# Extract the head of the data set\n",
    "comma_use.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-neutral",
   "metadata": {},
   "source": [
    "For more on this dataset see [here](https://fivethirtyeight.com/features/elitist-superfluous-or-popular-we-polled-americans-on-the-oxford-comma/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the count of each unique value in column 'In your opinion, which sentence is more gramatically correct?'\n",
    "comma_use['In your opinion, which sentence is more gramatically correct?'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-corpus",
   "metadata": {},
   "source": [
    "Cleaning the data before creating the dummy variables / columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dimension of the data frame\n",
    "comma_use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any missing values in the data frame\n",
    "comma_use.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "comma_use.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-gallery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dimension again after dropping the rows with missing values\n",
    "comma_use.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-redhead",
   "metadata": {},
   "source": [
    "Using **sklearn**'s OneHotEncoder to create the dummy columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder that creates K - 1 dummy columns\n",
    "ohe = OneHotEncoder(drop = 'first')\n",
    "\n",
    "# Transform all categorical variables into dummy columns except 'RespondentID'\n",
    "comma_trans = ohe.fit_transform(comma_use.drop('RespondentID', axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-works",
   "metadata": {},
   "source": [
    "Could we have used ```pd.get_dummies()``` instead?\n",
    "\n",
    "Well, yes. And in fact ```get_dummies()``` is in some ways easier; for one thing, it's built right into Pandas. But there are drawbacks with it as well. The main advantage of the `sklearn` tool is that it stores information about the columns and creates a persistent function that can be used on future data of the same form. See [this page](https://stackoverflow.com/questions/36631163/pandas-get-dummies-vs-sklearns-onehotencoder-what-are-the-pros-and-cons) for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas get_dummies() function to create dummy columns\n",
    "pd.get_dummies(comma_use.drop('RespondentID', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-bicycle",
   "metadata": {},
   "source": [
    "So what did the encoder do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-syndication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the transformed object\n",
    "comma_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numpy matrix object using the todense() method\n",
    "comma_trans.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-northeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the feature names of the transformed matrix\n",
    "ohe.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-productivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the matrix and feature names into a Pandas data frame\n",
    "comma_df = pd.DataFrame(comma_trans.todense(), columns=ohe.get_feature_names())\n",
    "comma_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-estonia",
   "metadata": {},
   "source": [
    "# Multiple Regression in `statsmodels`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-cinema",
   "metadata": {},
   "source": [
    "Let's build a multiple regression with `statsmodels`. Let's start with a toy model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "from scipy import stats as stats\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a 5 random variables\n",
    "centers = np.arange(1, 6)\n",
    "preds = np.array([stats.norm(loc=center, scale=3).rvs(200) for center in centers]).T\n",
    "preds_df = pd.DataFrame(preds, columns=[f'var{center}' for center in centers])\n",
    "\n",
    "# Set the target values depends on the 5 random variables\n",
    "target = preds_df['var1'] + 2*preds_df['var2'] + 3*preds_df['var3']\\\n",
    "    + 4*preds_df['var4'] + 5*preds_df['var5']\n",
    "target_df = pd.DataFrame(target, columns=['target'])\n",
    "\n",
    "# Combine the target column with the 5 random variables columns into a data frame\n",
    "df = pd.concat([preds_df, target_df], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the target 'y' and predictors 'X'\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the OLS model and fit the simulate data\n",
    "model = sm.OLS(endog = y, exog = X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary table\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-satisfaction",
   "metadata": {},
   "source": [
    "### Diamonds Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-tuner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'diamonds' data set from seaborn module\n",
    "data = sns.load_dataset('diamonds').drop(['cut', 'color', 'clarity'], axis = 1)\n",
    "\n",
    "# Extract the head of the data set\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the target and predictors for regression\n",
    "X, y = data.drop('price', axis=1), data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the OLS model and fit the data\n",
    "model2 = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the model summary table\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with some standard visualizations\n",
    "sm.graphics.plot_regress_exog(model2, 'carat', fig=plt.figure(figsize=(12, 8)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-marathon",
   "metadata": {},
   "source": [
    "### Check distribution of target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the target variable 'Price'\n",
    "y.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-daniel",
   "metadata": {},
   "source": [
    "The distribution of the target variable, price of the diamond, is obviously **skewed to the right**. This could affect the model performance and prediction result. One way to solve this problem is to scale the value by taking the log of the price column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the target by taking log of each price value\n",
    "y_scld = np.log(y)\n",
    "\n",
    "# Check the distribution of the scaled target \n",
    "y_scld.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-position",
   "metadata": {},
   "source": [
    "### Build model with log-scaled target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the OLS model again with the log-scaled target\n",
    "model3 = sm.OLS(y_scld, X).fit()\n",
    "\n",
    "# Print the model summary table\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with the standard visualization again\n",
    "sm.graphics.plot_regress_exog(model3, 'carat', fig=plt.figure(figsize=(12, 8)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-rocket",
   "metadata": {},
   "source": [
    "**Remember that $R^2$ can be negative!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependency\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two examples to demonstrate the negative R^2\n",
    "bad_pred = np.mean(y) * np.ones(len(y))\n",
    "worse_pred = (np.mean(y) + 1000) * np.ones(len(y))\n",
    "\n",
    "# Print the R^2 values for each example\n",
    "print(metrics.r2_score(y, bad_pred))\n",
    "print(metrics.r2_score(y, worse_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-cinema",
   "metadata": {},
   "source": [
    "# Putting it in Practice: Wine Dataset üç∑"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-yugoslavia",
   "metadata": {},
   "source": [
    "This dataset includes measurable attributes of different wines as well as their rated quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'wine' data set\n",
    "wine = pd.read_csv('data/wine.csv')\n",
    "\n",
    "# Extract the head of the data set\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data set info\n",
    "wine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-airline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the statistical summary of the data set\n",
    "wine.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-great",
   "metadata": {},
   "source": [
    "Imagine we want to attempt to estimate the perceived quality of a wine using these attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count each unique wine quality rating in the data set\n",
    "wine['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total of red wine / non-red wine in the data set\n",
    "wine['red_wine'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-cookie",
   "metadata": {},
   "source": [
    "## üß† **Knowledge Check**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-confidence",
   "metadata": {},
   "source": [
    "> Why are we using \"quality\" as the dependent variable (target)? Would it make sense for another feature to be the target instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-gothic",
   "metadata": {},
   "source": [
    "## Running the Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-picture",
   "metadata": {},
   "source": [
    "First, we'll separate the data into our predictors (X) and target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the target and predictors for regression\n",
    "wine_preds = wine.drop('quality', axis=1)\n",
    "wine_target = wine['quality']\n",
    "\n",
    "# Check the predictors data frame\n",
    "wine_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-agenda",
   "metadata": {},
   "source": [
    "Now we can perform our (multiple) linear regression! Since we already used `statsmodels`, let's use that again to fit the model and then check the summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sm.add_constant() to add constant term/y-intercept\n",
    "predictors = sm.add_constant(wine_preds)\n",
    "\n",
    "# Check if the constant column has been added to the data frame\n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the OLS model and fit the data\n",
    "model = sm.OLS(wine_target, predictors).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-database",
   "metadata": {},
   "source": [
    "> Alright! So we fitted our model! Take a look at the summary and look if you can understand the different parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary table\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-covering",
   "metadata": {},
   "source": [
    "# Scaling - The Missing & Helpful Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-celebration",
   "metadata": {},
   "source": [
    "When you looked at the summary after we did the linear regression, you might have noticed something interesting.\n",
    "\n",
    "Observing the coefficients, you might notice there are two relatively large coefficients and nearly rest are less than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-breed",
   "metadata": {},
   "source": [
    "## What's Going on Here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-andrew",
   "metadata": {},
   "source": [
    "In a word, it's useful to have all of our variables be on the same scale, so that the resulting coefficients are easier to interpret. If the scales of the variables are very different one from another, then some of the coefficients may end up on very large or very tiny scales.\n",
    "\n",
    "This happens since the coefficients will effectively attempt to \"shrink\" or \"expand\" the features before factoring their importance to the model.\n",
    "\n",
    "![](img/shrinkinator.jpeg)\n",
    "\n",
    "This can make it more difficult for interpretation and identifying coefficients with the most \"effect\" on the prediction.\n",
    "\n",
    "For more on this, see [this post](https://stats.stackexchange.com/questions/32649/some-of-my-predictors-are-on-very-different-scales-do-i-need-to-transform-them)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-course",
   "metadata": {},
   "source": [
    "## A Solution: Standard Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-gardening",
   "metadata": {},
   "source": [
    "One solution is to *scale* our features. There are a few ways to do this but we'll focus on **standard scaling**.\n",
    "\n",
    "When we do **standard scaling**, we're really scaling it to be the features' respective $z$-scores.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "- This tends to make values relatively small (mean value is at $0$ and one standard deviation $\\sigma$ from the mean is $1$).\n",
    "- Easier interpretation: larger coefficients tend to be more influential\n",
    "\n",
    "Next time, let's *scale* our columns as $z$-scores first. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-stationery",
   "metadata": {},
   "source": [
    "##  Redoing with Standard Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-eclipse",
   "metadata": {},
   "source": [
    "Let's try standard scaling the model with our wine dataset now.\n",
    "\n",
    "*Z*-Score Formula:\n",
    "\n",
    "$$z = \\frac{x - \\bar{x}}{sd(x)}$$\n",
    "\n",
    "where \n",
    "\n",
    "- $x$ is the actual value of $x$\n",
    "- $\\bar{x}$ is the average of $x$\n",
    "- $sd(x)$ is the sample standard deviation of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-howard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are scaling all colmuns using the z-scores formula\n",
    "wine_preds_scaled = (wine_preds - np.mean(wine_preds)) / np.std(wine_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the statistial summary of the scaled data set\n",
    "wine_preds_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the model with the standardized data\n",
    "predictors = sm.add_constant(wine_preds_scaled)\n",
    "model = sm.OLS(wine_target, predictors).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-holmes",
   "metadata": {},
   "source": [
    "> Check how well this model did with the one before scaling. Does it perform any differently?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-payment",
   "metadata": {},
   "source": [
    "Type *Markdown* and LateX: $\\alpha^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "broken-employment",
   "metadata": {},
   "source": [
    "## üß† **Knowledge Check**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-ridge",
   "metadata": {},
   "source": [
    "> After standard scaling, what would it mean when all the $x_i$ are all $0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-pocket",
   "metadata": {},
   "source": [
    "## üß† **Knowledge Check**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-feeding",
   "metadata": {},
   "source": [
    "### Follow-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-order",
   "metadata": {},
   "source": [
    "> What does this mean for the constant term $\\hat{\\beta}_0$? Could we check this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-maker",
   "metadata": {},
   "source": [
    "# Multiple Regression in Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-thirty",
   "metadata": {},
   "source": [
    "It's great that we tried out multiple linear regression with `statsmodels`; now let's try it with `sklearn`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-effect",
   "metadata": {},
   "source": [
    "## Scale the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a StandardScaler object to scale our data for us.\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll apply it to our data by using the .fit() and .transform() methods.\n",
    "ss.fit(wine_preds)\n",
    "wine_preds_st_scaled = ss.transform(wine_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-harvest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the scaling worked about the same as when we did it by hand\n",
    "np.allclose(wine_preds_st_scaled, wine_preds_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_preds_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the mean of the target 'wine quality'\n",
    "wine_target.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-rochester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first 5 scaled observations\n",
    "wine_preds_st_scaled[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-scope",
   "metadata": {},
   "source": [
    "## Fit the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-clothing",
   "metadata": {},
   "source": [
    "Now we can fit a `LinearRegression` object to our training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the OLS Regression object\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fit the training data using the OLS regression object\n",
    "lr.fit(wine_preds_st_scaled, wine_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the coefficient values using the .coef_ attribute\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the intercept coefficient\n",
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the R^2 score of the model\n",
    "lr.score(wine_preds_st_scaled, wine_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the training data set to predict the target values\n",
    "y_hat = lr.predict(wine_preds_st_scaled)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-capacity",
   "metadata": {},
   "source": [
    "All that's left is to evaluate our model to see how well it did!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-thumbnail",
   "metadata": {},
   "source": [
    "## Evaluate Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-balance",
   "metadata": {},
   "source": [
    "### Observing Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-tulsa",
   "metadata": {},
   "source": [
    "We can check the residuals like we would for a simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the predicted values array\n",
    "y_hat = lr.predict(wine_preds_st_scaled)\n",
    "\n",
    "# Calculate the residual (Actual - Predicted)\n",
    "resid = (wine_target - y_hat)\n",
    "\n",
    "# Display the residual across all predicted values\n",
    "plt.scatter(x=range(y_hat.shape[0]),y=resid, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-mixer",
   "metadata": {},
   "source": [
    "### Sklearn Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-munich",
   "metadata": {},
   "source": [
    "The metrics module in sklearn has a number of metrics that we can use to measure the accuracy of our model, including the $R^2$ score, the mean absolute error and the mean squared error. Note that the default 'score' on our model object is the $R^2$ score. Let's go back to our wine dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the R^2 of the model\n",
    "metrics.r2_score(wine_target, lr.predict(wine_preds_st_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-carroll",
   "metadata": {},
   "source": [
    "Let's make sure this metric is properly calibrated. If we put simply $\\bar{y}$ as our prediction, then we should get an $R^2$ score of *0*. And if we predict, say, $\\bar{y} + 1$, then we should get a *negative* $R^2$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculat the average target value\n",
    "avg_quality = np.mean(wine_target)\n",
    "\n",
    "# Calculate the total number of observations\n",
    "num = len(wine_target)\n",
    "\n",
    "# Check the R^2 with the average of the target values\n",
    "metrics.r2_score(wine_target, avg_quality * np.ones(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the R^2 with average of the target values + 1\n",
    "metrics.r2_score(wine_target, (avg_quality + 1) * np.ones(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the mean absolute error (MAE)\n",
    "metrics.mean_absolute_error(wine_target, lr.predict(wine_preds_st_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the mean squared error (MSE)\n",
    "metrics.mean_squared_error(wine_target, lr.predict(wine_preds_st_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-elite",
   "metadata": {},
   "source": [
    "# Level Up: Deeper Evaluation of Wine Data Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-medium",
   "metadata": {},
   "source": [
    "One thing we could have investigated from our [model on the Wine Data](#Multiple-Regression-in-Scikit-Learn) is how our predictions $\\hat{y}$ match with the actual target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-characteristic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the predicted wine quality\n",
    "sns.histplot(y_hat,kde=True,fill=False,stat='density',color='red')\n",
    "sns.histplot(wine_target,discrete=True,stat='density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-expense",
   "metadata": {},
   "source": [
    "So there's a slight issue with our model; the linear regression believes the target values are on a continuum. We know that's not true from the data. \n",
    "\n",
    "An easy fix is to round the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rounding the target values\n",
    "y_hat_rounded = np.round(y_hat)\n",
    "np.unique(y_hat_rounded, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-complaint",
   "metadata": {},
   "source": [
    "Plotting the distribution is a lot more meaningful if we require targets to be integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the predicted rounded values\n",
    "sns.histplot(np.round(y_hat),fill=False,discrete=True,stat='density',color='red')\n",
    "sns.histplot(wine_target,discrete=True,alpha=0.3,stat='density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-executive",
   "metadata": {},
   "source": [
    "Note that our $R^2$ metric will be worse. This makes sense since we found a \"line of best fit\" that predicts continuous values. \n",
    "\n",
    "If the better option was _integer_ predictions, it would have predicted that instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the R^2 with the rounded values\n",
    "metrics.r2_score(wine_target, y_hat_rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-harvard",
   "metadata": {},
   "source": [
    "You must decide yourself if this is worth doing or if a different model makes more sense (we'll see more models in future lectures)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-letter",
   "metadata": {},
   "source": [
    "# Level Up: Regression with Categorical Features with the Comma Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column names of the comma data set\n",
    "comma_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll try to predict the first column of df: the extent to which\n",
    "# the person accepts the sentence\n",
    "# without the Oxford comma as more grammatically correct.\n",
    "\n",
    "comma_target = comma_df['x0_It\\'s important for a person to be honest, kind, and loyal.']\n",
    "\n",
    "comma_predictors = comma_df[['x8_30-44',\n",
    "       'x8_45-60', 'x8_> 60', 'x9_$100,000 - $149,999',\n",
    "       'x9_$150,000+', 'x9_$25,000 - $49,999', 'x9_$50,000 - $99,999']]\n",
    "\n",
    "comma_lr = LinearRegression()\n",
    "\n",
    "comma_lr.fit(comma_predictors, comma_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2 of the model\n",
    "comma_lr.score(comma_predictors, comma_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE of the model prediction\n",
    "metrics.mean_squared_error(wine_target, y_hat_rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated coefficients\n",
    "comma_lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between x0 and other predictors\n",
    "comma_df.corr()['x0_It\\'s important for a person to be honest, kind, and loyal.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-dealing",
   "metadata": {},
   "source": [
    "For more on the interpretation of regression coefficients for categorical variables, see [Erin's repo](https://github.com/hoffm386/coefficients-of-dropped-categorical-variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-parish",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
