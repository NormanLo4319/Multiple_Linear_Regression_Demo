{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prescription-cinema",
   "metadata": {},
   "source": [
    "# Putting it in Practice: Wine Dataset 🍷"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-yugoslavia",
   "metadata": {},
   "source": [
    "This dataset includes measurable attributes of different wines as well as their rated quality. We are going to fit the data with multiple linear regression model using ```statsmodel``` library.\n",
    "\n",
    "General Workflow:\n",
    "1. Checking the data set to make sure it is ready to fit into the model\n",
    "2. Clean or manipulate the data if needed\n",
    "3. Create the predictors (independent) and target (dependent) matrices\n",
    "4. Add the constant column to the predictor matrix using ```sm.add_constant()```\n",
    "5. Fit the data using ```sm.OLS()```\n",
    "6. Print the regression summary table using ```.summary()``` method\n",
    "\n",
    "Note: If we were running the ```sm.OLS()``` without creating the constant column in the predictors dataframe, it will return a different result, which has a completely different interpretation.\n",
    "\n",
    "Model without Constant:\n",
    "$$y = \\beta1 x_1 + \\beta_2 x_2 + ... \\beta_k x_k$$\n",
    "\n",
    "Model with Constant:\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... \\beta_k x_k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'wine' data set\n",
    "wine = pd.read_csv('data/wine.csv')\n",
    "\n",
    "# Extract the head of the data set\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data set info\n",
    "wine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-airline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the statistical summary of the data set\n",
    "wine.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-great",
   "metadata": {},
   "source": [
    "Imagine we want to attempt to estimate the perceived quality of a wine using these attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count each unique wine quality rating in the data set\n",
    "wine['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total of red wine / non-red wine in the data set\n",
    "wine['red_wine'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-cookie",
   "metadata": {},
   "source": [
    "## 🧠 **Knowledge Check**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-confidence",
   "metadata": {},
   "source": [
    "> Why are we using \"quality\" as the dependent variable (target)? Would it make sense for another feature to be the target instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-gothic",
   "metadata": {},
   "source": [
    "## Running the Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-picture",
   "metadata": {},
   "source": [
    "First, we'll separate the data into our predictors (X) and target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the target and predictors for regression\n",
    "wine_preds = wine.drop('quality', axis = 1)\n",
    "wine_target = wine['quality']\n",
    "\n",
    "# Check the predictors data frame\n",
    "wine_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-agenda",
   "metadata": {},
   "source": [
    "Now we can perform our (multiple) linear regression! Since we already used `statsmodels`, let's use that again to fit the model and then check the summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sm.add_constant() to add constant term/y-intercept\n",
    "predictors = sm.add_constant(wine_preds)\n",
    "\n",
    "# Check if the constant column has been added to the data frame\n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the OLS model and fit the data\n",
    "model = sm.OLS(wine_target, predictors).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-database",
   "metadata": {},
   "source": [
    "> Alright! So we fitted our model! Take a look at the summary and look if you can understand the different parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary table\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-covering",
   "metadata": {},
   "source": [
    "# Scaling - The Missing & Helpful Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-celebration",
   "metadata": {},
   "source": [
    "When you looked at the summary after we did the linear regression, you might have noticed something interesting.\n",
    "\n",
    "Observing the coefficients, you might notice there are two relatively large coefficients and nearly rest are less than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-breed",
   "metadata": {},
   "source": [
    "## What's Going on Here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-andrew",
   "metadata": {},
   "source": [
    "In a word, it's useful to have all of our variables be on the same scale, so that the resulting coefficients are easier to interpret. If the scales of the variables are very different one from another, then some of the coefficients may end up on very large or very tiny scales.\n",
    "\n",
    "This happens since the coefficients will effectively attempt to \"shrink\" or \"expand\" the features before factoring their importance to the model.\n",
    "\n",
    "![](img/shrinkinator.jpeg)\n",
    "\n",
    "This can make it more difficult for interpretation and identifying coefficients with the most \"effect\" on the prediction.\n",
    "\n",
    "For more on this, see [this post](https://stats.stackexchange.com/questions/32649/some-of-my-predictors-are-on-very-different-scales-do-i-need-to-transform-them)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-course",
   "metadata": {},
   "source": [
    "## A Solution: Standard Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-gardening",
   "metadata": {},
   "source": [
    "One solution is to *scale* our features. There are a few ways to do this but we'll focus on **standard scaling**. For more about scaling the data, check out this [link](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/)\n",
    "\n",
    "When we do **standard scaling**, we're really scaling it to be the features' respective $z$-scores.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "- This tends to make values relatively small (mean value is at $0$ and one standard deviation $\\sigma$ from the mean is $1$).\n",
    "- Easier interpretation: larger coefficients tend to be more influential\n",
    "\n",
    "![](img/standard_scaling.png)\n",
    "\n",
    "Next time, let's *scale* our columns as $z$-scores first. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-stationery",
   "metadata": {},
   "source": [
    "##  Redoing with Standard Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-eclipse",
   "metadata": {},
   "source": [
    "Let's try standard scaling the model with our wine dataset now.\n",
    "\n",
    "*Z*-Score Formula:\n",
    "\n",
    "$$z = \\frac{x - \\bar{x}}{sd(x)}$$\n",
    "\n",
    "where \n",
    "\n",
    "- $x$ is the actual value of $x$\n",
    "- $\\bar{x}$ is the average of $x$\n",
    "- $sd(x)$ is the sample standard deviation of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-howard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are scaling all colmuns using the z-scores formula\n",
    "wine_preds_scaled = (wine_preds - np.mean(wine_preds)) / np.std(wine_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the statistial summary of the scaled data set\n",
    "wine_preds_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the model with the standardized data\n",
    "predictors_scaled = sm.add_constant(wine_preds_scaled)\n",
    "model = sm.OLS(wine_target, predictors_scaled).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-holmes",
   "metadata": {},
   "source": [
    "> Check how well this model did with the one before scaling. Does it perform any differently?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-employment",
   "metadata": {},
   "source": [
    "## 🧠 **Knowledge Check**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-ridge",
   "metadata": {},
   "source": [
    "> After standard scaling, what would it mean when all the $x_i$ are all $0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-pocket",
   "metadata": {},
   "source": [
    "## 🧠 **Knowledge Check**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-feeding",
   "metadata": {},
   "source": [
    "### Follow-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-order",
   "metadata": {},
   "source": [
    "> What does this mean for the constant term $\\hat{\\beta}_0$? Could we check this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-maker",
   "metadata": {},
   "source": [
    "# Multiple Regression in Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-thirty",
   "metadata": {},
   "source": [
    "It's great that we tried out multiple linear regression with `statsmodels`; now let's try it with `sklearn`!\n",
    "\n",
    "The Sklearn library provides us with a Linear Regression model that will fit a line to our data. Sklearn follows a consistent API where you define a model object, fit the model to the data, and then make predictions with the model.\n",
    "\n",
    "![sklearn](img/sklearn_api.png)\n",
    "\n",
    "Here are the modules we are going to import for linear regression, scaler, and statistical metrics.\n",
    "\n",
    "``` Python\n",
    "# Import dependencies\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import sklearn.metrics as metrics\n",
    "```\n",
    "\n",
    "For standard scaling with *sklearn**, we follow the following workflow.\n",
    "\n",
    "``` Python\n",
    "# First create a StandardScaler object\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Use the .fit() and .transform() methods to apply to the data\n",
    "data_scaler = ss.fit(data)\n",
    "data_scaled = ss.transform(data)\n",
    "```\n",
    "\n",
    "Note: The object returned from ```.transform()``` is a *Numpy* array / matrix, instead of *Pandas* dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-effect",
   "metadata": {},
   "source": [
    "## Scale the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a StandardScaler object to scale our data for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll apply it to our data by using the .fit() and .transform() methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous scaled data was stored in Pandas dataframe\n",
    "wine_preds_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-rochester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn StandardScaler() returns Numpy array / matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the scaling worked about the same as when we did it by hand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-scope",
   "metadata": {},
   "source": [
    "## Fit the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-clothing",
   "metadata": {},
   "source": [
    "Now we can fit a `LinearRegression` object to our training data!\n",
    "\n",
    "Here are some standard code for using *sklearn* regression model:\n",
    "\n",
    "``` Python\n",
    "# Create the linear regression object\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fitting the data using the linear regression object\n",
    "lr.fit(X, y)\n",
    "\n",
    "# Extract estimated coefficients\n",
    "lr.coef_\n",
    "\n",
    "# Extract estimated intercept coefficient\n",
    "lr.intcept_\n",
    "\n",
    "# Extract the R^2 of the model\n",
    "lr.score(X, y)\n",
    "\n",
    "# Make prediction of the targe value using the predictors\n",
    "lr.predict(X)\n",
    "```\n",
    "\n",
    "Note: We can pass in either *Pandas* dataframe or *Numpy* array to the regression model. Also, the *sklearn* regression model do not require adding constant column to the **predictors** matrix or dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the linear Regression object\n",
    "\n",
    "\n",
    "# Fit the training data using the linear regression object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the coefficient values using the .coef_ attribute\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the intercept coefficient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the R^2 score of the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the training data set to predict the target values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-capacity",
   "metadata": {},
   "source": [
    "All that's left is to evaluate our model to see how well it did!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-thumbnail",
   "metadata": {},
   "source": [
    "## Evaluate Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-balance",
   "metadata": {},
   "source": [
    "### Observing Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-tulsa",
   "metadata": {},
   "source": [
    "We can check the residuals like we would for a simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the predicted values array\n",
    "y_hat = lr.predict(wine_preds_st_scaled)\n",
    "\n",
    "# Calculate the residual (Actual - Predicted)\n",
    "resid = (wine_target - y_hat)\n",
    "\n",
    "# Display the residual across all predicted values\n",
    "plt.scatter(x=range(y_hat.shape[0]),y=resid, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-mixer",
   "metadata": {},
   "source": [
    "### Sklearn Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-munich",
   "metadata": {},
   "source": [
    "The metrics module in sklearn has a number of metrics that we can use to measure the accuracy of our model, including the $R^2$ score, the mean absolute error and the mean squared error. Note that the default 'score' on our model object is the $R^2$ score. Let's go back to our wine dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the R^2 of the model\n",
    "metrics.r2_score(wine_target, lr.predict(wine_preds_st_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-carroll",
   "metadata": {},
   "source": [
    "Let's make sure this metric is properly calibrated. If we put simply $\\bar{y}$ as our prediction, then we should get an $R^2$ score of *0*. And if we predict, say, $\\bar{y} + 1$, then we should get a *negative* $R^2$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculat the average target value\n",
    "avg_quality = np.mean(wine_target)\n",
    "\n",
    "# Calculate the total number of observations\n",
    "num = len(wine_target)\n",
    "\n",
    "# Check the R^2 with the average of the target values\n",
    "metrics.r2_score(wine_target, avg_quality * np.ones(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the R^2 with average of the target values + 1\n",
    "metrics.r2_score(wine_target, (avg_quality + 1) * np.ones(num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-african",
   "metadata": {},
   "source": [
    "Mean Absolute Error:\n",
    "\n",
    "$$MAE = \\frac{\\sum_{i=1}^{n}|\\hat{y_i} - y_i|}{n}$$\n",
    "\n",
    "where \n",
    "\n",
    "- $\\hat{y_i}$ is the predicted value\n",
    "- $y_i$ is the true value\n",
    "- $n$ is the total number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the mean absolute error (MAE)\n",
    "metrics.mean_absolute_error(wine_target, lr.predict(wine_preds_st_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-postcard",
   "metadata": {},
   "source": [
    "Mean Squared Error:\n",
    "\n",
    "$$MSE = \\frac{\\sum_{i=1}^{n}(\\hat{y_i} - y_i)^2}{n}$$\n",
    "\n",
    "where \n",
    "\n",
    "- $\\hat{y_i}$ is the predicted value\n",
    "- $y_i$ is the true value\n",
    "- $n$ is the total number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the mean squared error (MSE)\n",
    "metrics.mean_squared_error(wine_target, lr.predict(wine_preds_st_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-injection",
   "metadata": {},
   "source": [
    "## Practice with a Partner: (20 minutes)\n",
    "\n",
    "We have a interesting data set that study our brain. In this exercise we are trying to use multiple features to predict brain **weight**.\n",
    "\n",
    "Source: R.J. Gladstone (1905). \"A Study of the Relations of the Brain to the Size of the Head\", Biometrika, Vol. 4, pp105-123\n",
    "\n",
    "Description: Brain weight (grams) and head size (cubic cm) for 237 adults classified by gender and age group.\n",
    "\n",
    "Variables/Columns:\n",
    "\n",
    "GENDER: Gender  Male or Female  \n",
    "AGE: Age Range  20-46 or 46+  \n",
    "SIZE: Head size ($cm^3$)  21-24  \n",
    "WEIGHT: Brain weight (grams)  29-32 \n",
    "\n",
    "### Objectives:\n",
    "- Follow the standard workflow, such as checking the data type, cleaning the data, prepare data for regression model.\n",
    "- Create dummy variables for the categorical data using either Pandas ```pd.get_dummy()``` or sklearn ```OneHotEncoder()```.\n",
    "- Scale the data using sklearn ```standardScaler()```.\n",
    "- Fit the data in multiple linear regression using sklearn module.\n",
    "- Evaluate the model using different metrics, $R^2$, MAE, MSE.\n",
    "\n",
    "Follow the instructions and complete a coding exercise with a partner in the class (in the random breakout room). Instructor will go into the rooms to help answering questions from students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file into a pandas DataFrame\n",
    "brain = pd.read_csv('data/brain_categorical.csv')\n",
    "brain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-freeware",
   "metadata": {},
   "source": [
    "### Step 1: Checking the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .info() method to check the data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-gossip",
   "metadata": {},
   "source": [
    "Question:\n",
    "\n",
    "1. Do we have any missing data?\n",
    "2. What are the data type for each variable (column)?\n",
    "3. Do we need to clean the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .describe() method to print the statistical summary of the data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-phoenix",
   "metadata": {},
   "source": [
    "Question:\n",
    "\n",
    "1. Why are we see the statistical summary returning only two columns (size and weight)?\n",
    "\n",
    "Let's check the count of each category (level) in both categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of male and female\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of different age groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-sending",
   "metadata": {},
   "source": [
    "Next, let's create the predictors and target dataframes.\n",
    "\n",
    "Example:\n",
    "\n",
    "``` Python\n",
    "# Create predictors dataframe\n",
    "predictors = data.drop('target_name', axis=1)\n",
    "\n",
    "# Create target dataframe\n",
    "target = data['target_name']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the predictors and target dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-avenue",
   "metadata": {},
   "source": [
    "### Step 2: Create dummy variables for categorical data\n",
    "\n",
    "Now that we have studied the data set and knowing that there are two categorical variables (columns). We need to create dummy columns.\n",
    "\n",
    "Feel free to use either *Pandas* ```pd.get_dummies()``` or *sklearn* ```OneHotEncoder()``` for this task.\n",
    "\n",
    "Example:\n",
    "\n",
    "``` Python\n",
    "# Create dummy columns with Pandas function\n",
    "data_encoded = pd.get_dummies(data, drop_first = True)\n",
    "\n",
    "# Create dummy columns with sklearn OneHotEncoder\n",
    "ohe = OneHotEncoder(drop = 'first')\n",
    "data_trans = ohe.fit_transform(data['categorical_columns'])\n",
    "data_encoded = pd.DataFrame(data_trans.todense(), columns = ohe.get_feature_names())\n",
    "data_encoded.join(data['numerical_columns'])\n",
    "data_encoded\n",
    "```\n",
    "\n",
    "Note: *Remember when we are fitting / training the data with the OLS regression model, we need to exclude one level from each categorical variable as the reference level.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-treasurer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the categorical columns with Pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the categorical columns with sklearn OneHotEncoder (optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-regression",
   "metadata": {},
   "source": [
    "Check the encoded predictors dataframe to make sure the dummy columns are created and the original columns are removed.\n",
    "\n",
    "Let's run the sklearn OLS with the encoded data and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-penny",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the OLS Regression object\n",
    "\n",
    "\n",
    "# Fit the training data using the OLS regression object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the coefficient values using the .coef_ attribute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the intercept coefficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-bankruptcy",
   "metadata": {},
   "source": [
    "### Step 3: Scaling the data\n",
    "\n",
    "If we look at the estimated coefficents, there are two relatively large coefficients (gender_male and age_46+) and a cofficient (size) less than 1. \n",
    "\n",
    "Remember, we can use the sklearn ```StandardScaler()``` to standardize the data, so the variables will be on the same scale.\n",
    "\n",
    "Example:\n",
    "\n",
    "``` Python\n",
    "# Create the StandardScaler object\n",
    "ss = StandardScaler()\n",
    "\n",
    "## Apply it to our data by using the .fit() and .transform() methods\n",
    "data_scaler = ss.fit(data)\n",
    "data_scaled = data_scaler.transform(data)\n",
    "```\n",
    "\n",
    "Note: *The StandardScaler is expecting a 2D array for input. If we are scaling our target variable, we need to reshape the dataframe using ```target.values.reshape(-1, 1)``` in the .fit() and .transfrom() functions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the StandardScaler object\n",
    "\n",
    "\n",
    "# Apply it the predictors and target by using the .fit() and .transform() methods\n",
    "\n",
    "\n",
    "# Create the scaled dataframes\n",
    "\n",
    "\n",
    "# Print the scaled predictors and target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-manual",
   "metadata": {},
   "source": [
    "### Step 4: Fitting the mulitiple regression model\n",
    "\n",
    "Let's run the sklearn OLS regrssion on the scaled data again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the OLS Regression object\n",
    "\n",
    "\n",
    "# Fit the training data using the OLS regression object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the coefficient values using the .coef_ attribute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the intercept coefficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-liver",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate the model\n",
    "\n",
    "Let's evaluate the model with residual graph and different metrics, such as $R^2$, MAE, and MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the predicted values array\n",
    "\n",
    "\n",
    "# Calculate the residual (Actual - Predicted)\n",
    "\n",
    "\n",
    "# Display the residual across all predicted values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the R^2 score of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the mean absolute error (MAE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-concentration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the mean squared error (MSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-element",
   "metadata": {},
   "source": [
    "**Final Challenge:**\n",
    "\n",
    "Let's use statsmodel OLS regression to confirm our results from the previous exercise. Run two OLS regression, one with the encoded data and one with the scaled data.\n",
    "\n",
    "Check the coefficient and $R^2$ values. They should be the same as the sklearn OLS regression models.  If they are not, something is wrong!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the OLS model and fit the encoded data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-institute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the OLS model and fit the scaled data\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
